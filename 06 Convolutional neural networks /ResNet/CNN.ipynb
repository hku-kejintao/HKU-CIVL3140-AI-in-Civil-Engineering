{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdb54ce-6b4f-423e-971a-faf57a8f5aef",
   "metadata": {},
   "source": [
    "# Tutorial 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Python Tutorial on Convolutional Neural Networks (CNNs)! In this comprehensive guide, we will dive into one of the most important and powerful deep learning architectures for image processing - CNNs. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.\n",
    "\n",
    "CNNs have revolutionized computer vision tasks, such as image classification, object detection, segmentation, and more. In this tutorial, we will explore the fundamental concepts of CNNs, their architecture, and the mathematics behind their working. We will also walk through the implementation of CNNs in Python using popular deep learning libraries.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into this tutorial, it is recommended to have a solid understanding of the following topics:\n",
    "\n",
    "- Python programming fundamentals\n",
    "- Basics of machine learning and neural networks\n",
    "- Linear algebra and calculus concepts - Understanding matrices, vectors, and derivatives will be beneficial for grasping CNNs.\n",
    "\n",
    "Knowledge of libraries like NumPy, PyTorch, and Matplotlib will be helpful, as we will use them properly in our implementations and visualizations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the fundamental building blocks of Convolutional Neural Networks (CNNs).\n",
    "- Comprehend the concept of convolution and its role in learning local patterns.\n",
    "- Implement a basic CNN in Python using PyTorch, a popular deep learning framework.\n",
    "- Train the CNN model on a dataset for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc947af0-a9e7-4eb2-837a-3a29b837cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efd635-798f-425e-aee2-ef383991b7ef",
   "metadata": {},
   "source": [
    "## CNN model definition\n",
    "\n",
    "```python\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, classes)\n",
    "```\n",
    "\n",
    "- The code defines a custom CNN class that inherits from `nn.Module`, which is the base class for all neural network modules in PyTorch.\n",
    "- The `__init__` method is the constructor that initializes the layers of the CNN model.\n",
    "- The CNN model has three convolutional layers, each followed by a batch normalization layer.\n",
    "- The `nn.Conv2d` layers define the convolutional operations. The first layer takes input channels (3 for RGB images), has 32 output channels (32 filters), a kernel size of 3x3, a stride of 1, and padding of 1.\n",
    "- The `nn.BatchNorm2d` layers perform batch normalization, which helps stabilize training by normalizing the inputs to each layer.\n",
    "- The `nn.MaxPool2d` layer is used for max pooling with a kernel size of 2x2 and a stride of 2, effectively reducing the spatial dimensions by half.\n",
    "- The final `nn.Linear` layer is the fully connected layer that maps the output of the convolutional layers to the number of classes in the dataset.\n",
    "\n",
    "```python\n",
    "    def forward(self, x):  # input: batch_size * 3 * 64 * 64\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.conv1(x)  # batch_size * 32 * 64 * 64\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 32 * 32 * 32\n",
    "\n",
    "        x = self.conv2(x)  # batch_size * 64 * 32 * 32\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 64 * 16 * 16\n",
    "\n",
    "        x = self.conv3(x)  # batch_size * 128 * 16 * 16\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 128 * 8 * 8\n",
    "\n",
    "        x = x.view(batch_size, -1)  # batch_size * (128 * 8 * 8)\n",
    "        x = self.fc1(x)  # batch_size * 10\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "- The `forward` method defines the forward pass of the CNN model, which takes the input tensor `x` (batch of images) as an argument.\n",
    "- The input tensor has the shape `batch_size * 3 * 64 * 64`, where batch_size is the number of images in the batch, `3` is the number of channels (RGB), and `64 * 64` is the image size.\n",
    "- The tensor `x` undergoes a series of operations: convolution, batch normalization, ReLU activation, and max pooling in each convolutional block.\n",
    "- After the convolutional blocks, the tensor is flattened using `x.view(batch_size, -1)` to convert it into a 1D vector to be passed through the fully connected layer.\n",
    "- Finally, the tensor `x` is passed through the fully connected layer `self.fc1` to obtain the final output tensor of shape `batch_size * classes` (where `classes` is the number of output classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a7d3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, classes)\n",
    "       \n",
    "\n",
    "    def forward(self, x):  # input: batch_size * 3 * 64 * 64\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.conv1(x) # batch_size * 32 * 64 * 64\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 32 * 32 * 32\n",
    "        \n",
    "        x = self.conv2(x) # batch_size * 64 * 32 * 32\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 64 * 16 * 16\n",
    "        \n",
    "        x = self.conv3(x) # batch_size * 128 * 16 * 16\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 128 * 8 * 8\n",
    "        \n",
    "        x = x.view(batch_size, -1) # batch_size * (128 * 8 * 8)\n",
    "        x = self.fc1(x)            # batch_size * 10\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc3ba2-f8c9-42b5-adbd-6840cfa8c45b",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Now we define a function named `load_data` that is used to load and preprocess image data for a CNN model. It utilizes PyTorch's `datasets.ImageFolder` and `DataLoader` classes to handle data loading and batching.\n",
    "\n",
    "```python\n",
    "def load_data(data_dir=\"./data/\", input_size=64, batch_size=36):\n",
    "    # data augmentation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "```\n",
    "\n",
    "- The `data_transforms` dictionary defines two different data transformations: one for training data (`'train'`) and another for test/validation data (`'test'`).\n",
    "- For training data, the `transforms.RandomResizedCrop` randomly crops and resizes the input image to `input_size`. It also applies random horizontal flips using `transforms.RandomHorizontalFlip`.\n",
    "- For test/validation data, the `transforms.Resize` resizes the input image to `input_size`, and `transforms.CenterCrop` performs center cropping to ensure the same size for all test images.\n",
    "- In both cases, the `transforms.ToTensor` converts the images to PyTorch tensors, and `transforms.Normalize` standardizes the pixel values using mean `[0.485, 0.456, 0.406]` and standard deviation `[0.229, 0.224, 0.225]`. These normalization values are commonly used with pre-trained models.\n",
    "\n",
    "Then we create train data and validation data:\n",
    "\n",
    "```python\n",
    "    image_dataset_train = datasets.ImageFolder(os.path.join(data_dir, '2-Medium-Scale'), data_transforms['train'])\n",
    "    image_dataset_valid = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "```\n",
    "\n",
    "- The `datasets.ImageFolder` class is used to create datasets from image folders. It expects the data to be organized in subfolders, with each subfolder representing a class label.\n",
    "- `image_dataset_train` and `image_dataset_valid` are created using `datasets.ImageFolder`. They represent the training and validation datasets, respectively.\n",
    "- The `os.path.join` function is used to construct the paths to the data directories.\n",
    "\n",
    "After generating data, we also need to convert them into `Dataloader` class, which are ready to be used in training and validation loops of a CNN model.\n",
    "\n",
    "```python\n",
    "    train_loader = DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(image_dataset_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "```\n",
    "\n",
    "- The `DataLoader` class is used to create data loaders that provide batches of data during training and validation.\n",
    "- `train_loader` and `valid_loader` are created using DataLoader, and they represent the data loaders for the training and validation datasets, respectively.\n",
    "- `batch_size` is specified as an argument to the data loaders, determining the number of samples in each batch.\n",
    "- `shuffle=True` for the training data loader ensures that the data is shuffled at the beginning of each epoch, which is essential for stochastic gradient descent.\n",
    "- `shuffle=False` for the validation data loader means the data will not be shuffled to maintain the order of the data for evaluation.\n",
    "- `num_workers=0` specifies the number of subprocesses used for data loading. Setting it to 0 means the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cca5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "## the mean and standard variance of imagenet dataset\n",
    "## mean_vals = [0.485, 0.456, 0.406]\n",
    "## std_vals = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_data(data_dir = \"./data/\",input_size = 64,batch_size = 36):\n",
    "    # data augmentation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    ## Load dataset\n",
    "    ## For other tasks, you may need to modify the data dir or even rewrite some part of 'data.py'\n",
    "    image_dataset_train = datasets.ImageFolder(os.path.join(data_dir, '2-Medium-Scale'), data_transforms['train'])\n",
    "    image_dataset_valid = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "\n",
    "    train_loader = DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(image_dataset_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5b856-5b2a-490d-b336-29d7b318171f",
   "metadata": {},
   "source": [
    "## Model Training Definition\n",
    "\n",
    "The train_model function takes the following arguments:\n",
    "\n",
    "- `model`: The neural network model to be trained.\n",
    "- `train_loader`: The data loader for the training dataset.\n",
    "- `valid_loader`: The data loader for the validation dataset.\n",
    "- `criterion`: The loss function to measure the model's performance during training.\n",
    "- `optimizer`: The optimization algorithm to update the model's parameters.\n",
    "- `num_epochs`: The number of training epochs (default value is 20).\n",
    "\n",
    "The principle can be shown as follows:\n",
    "\n",
    "- During each training epoch, the model performs forward and backward passes on the training data and updates its parameters using an optimizer, the input data (inputs) and target labels (labels) are moved to the appropriate device (e.g., GPU) if available.\n",
    "- The model's parameters gradients are zeroed (`optimizer.zero_grad()`), and a forward pass is performed to obtain predictions (`outputs`) on the input data.\n",
    "- The loss is calculated by comparing the predictions with the true labels (`criterion(outputs, labels)`).\n",
    "- The gradients are computed with respect to the loss (`loss.backward()`), and the model's parameters are updated using the optimizer (`optimizer.step()`).\n",
    "- After each epoch, the function evaluates the model's performance on the validation dataset.\n",
    "- The average loss and accuracy for both training and validation datasets are computed and displayed for each epoch.\n",
    "- The function also saves the model with the best validation accuracy as `best_model.pt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b790500",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that: here we provide a basic solution for training and validation.\n",
    "## You can directly change it if you find something wrong or not good enough.\n",
    "\n",
    "def train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=20):        \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model\n",
    "        model.train(True)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # send the data to device (GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # prediction\n",
    "            loss = criterion(outputs, labels) # loss\n",
    "            _, predictions = torch.max(outputs, 1) # The class with maximal probability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = total_correct.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # test\n",
    "        model.train(False)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "        valid_loss = total_loss / len(valid_loader.dataset)\n",
    "        valid_acc = total_correct.double() / len(valid_loader.dataset)\n",
    "        \n",
    "        # Show the results\n",
    "        print('*' * 100)\n",
    "        print('epoch:{:d}/{:d}'.format(epoch, num_epochs))\n",
    "        print(\"training: loss:   {:.4f}, accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        print(\"validation: loss: {:.4f}, accuracy: {:.4f}\".format(valid_loss, valid_acc))\n",
    "        \n",
    "        # save the best model\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "            torch.save(best_model, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a72ee6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "****************************************************************************************************\n",
      "epoch:0/50\n",
      "training: loss:   6.4479, accuracy: 0.1480\n",
      "validation: loss: 7.7156, accuracy: 0.2000\n",
      "****************************************************************************************************\n",
      "epoch:1/50\n",
      "training: loss:   5.1180, accuracy: 0.2820\n",
      "validation: loss: 10.2933, accuracy: 0.2700\n",
      "****************************************************************************************************\n",
      "epoch:2/50\n",
      "training: loss:   3.6913, accuracy: 0.3160\n",
      "validation: loss: 5.0701, accuracy: 0.2600\n",
      "****************************************************************************************************\n",
      "epoch:3/50\n",
      "training: loss:   3.0899, accuracy: 0.3180\n",
      "validation: loss: 3.8203, accuracy: 0.2800\n",
      "****************************************************************************************************\n",
      "epoch:4/50\n",
      "training: loss:   2.6755, accuracy: 0.3840\n",
      "validation: loss: 2.2697, accuracy: 0.3800\n",
      "****************************************************************************************************\n",
      "epoch:5/50\n",
      "training: loss:   2.1834, accuracy: 0.3920\n",
      "validation: loss: 2.1474, accuracy: 0.3900\n",
      "****************************************************************************************************\n",
      "epoch:6/50\n",
      "training: loss:   1.9609, accuracy: 0.4580\n",
      "validation: loss: 2.1636, accuracy: 0.4100\n",
      "****************************************************************************************************\n",
      "epoch:7/50\n",
      "training: loss:   1.6733, accuracy: 0.5380\n",
      "validation: loss: 1.4613, accuracy: 0.5100\n",
      "****************************************************************************************************\n",
      "epoch:8/50\n",
      "training: loss:   1.5791, accuracy: 0.5020\n",
      "validation: loss: 1.8864, accuracy: 0.4700\n",
      "****************************************************************************************************\n",
      "epoch:9/50\n",
      "training: loss:   1.6869, accuracy: 0.4740\n",
      "validation: loss: 2.0657, accuracy: 0.4400\n",
      "****************************************************************************************************\n",
      "epoch:10/50\n",
      "training: loss:   2.0562, accuracy: 0.4680\n",
      "validation: loss: 1.5794, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:11/50\n",
      "training: loss:   1.6903, accuracy: 0.5100\n",
      "validation: loss: 1.7793, accuracy: 0.4100\n",
      "****************************************************************************************************\n",
      "epoch:12/50\n",
      "training: loss:   1.5217, accuracy: 0.5220\n",
      "validation: loss: 1.3248, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:13/50\n",
      "training: loss:   1.4033, accuracy: 0.5400\n",
      "validation: loss: 1.4271, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:14/50\n",
      "training: loss:   1.4338, accuracy: 0.5140\n",
      "validation: loss: 1.4898, accuracy: 0.4800\n",
      "****************************************************************************************************\n",
      "epoch:15/50\n",
      "training: loss:   1.2736, accuracy: 0.5580\n",
      "validation: loss: 2.2862, accuracy: 0.4900\n",
      "****************************************************************************************************\n",
      "epoch:16/50\n",
      "training: loss:   1.3026, accuracy: 0.5640\n",
      "validation: loss: 1.2563, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:17/50\n",
      "training: loss:   1.1639, accuracy: 0.5760\n",
      "validation: loss: 1.2366, accuracy: 0.5800\n",
      "****************************************************************************************************\n",
      "epoch:18/50\n",
      "training: loss:   1.2194, accuracy: 0.5920\n",
      "validation: loss: 1.0950, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:19/50\n",
      "training: loss:   1.0888, accuracy: 0.6260\n",
      "validation: loss: 1.3845, accuracy: 0.5400\n",
      "****************************************************************************************************\n",
      "epoch:20/50\n",
      "training: loss:   1.1277, accuracy: 0.6100\n",
      "validation: loss: 1.2611, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:21/50\n",
      "training: loss:   1.0421, accuracy: 0.6580\n",
      "validation: loss: 1.0007, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:22/50\n",
      "training: loss:   1.0192, accuracy: 0.6400\n",
      "validation: loss: 1.1555, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:23/50\n",
      "training: loss:   1.0476, accuracy: 0.6400\n",
      "validation: loss: 1.4828, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:24/50\n",
      "training: loss:   0.9482, accuracy: 0.6720\n",
      "validation: loss: 1.1487, accuracy: 0.5900\n",
      "****************************************************************************************************\n",
      "epoch:25/50\n",
      "training: loss:   1.0442, accuracy: 0.6320\n",
      "validation: loss: 1.1240, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:26/50\n",
      "training: loss:   1.0979, accuracy: 0.6360\n",
      "validation: loss: 1.4370, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:27/50\n",
      "training: loss:   1.0086, accuracy: 0.6720\n",
      "validation: loss: 1.2886, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:28/50\n",
      "training: loss:   0.9608, accuracy: 0.6380\n",
      "validation: loss: 1.1931, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:29/50\n",
      "training: loss:   1.0308, accuracy: 0.6360\n",
      "validation: loss: 1.1818, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:30/50\n",
      "training: loss:   1.0672, accuracy: 0.6440\n",
      "validation: loss: 1.3000, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:31/50\n",
      "training: loss:   0.9262, accuracy: 0.6880\n",
      "validation: loss: 1.0577, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:32/50\n",
      "training: loss:   1.0131, accuracy: 0.6760\n",
      "validation: loss: 1.2925, accuracy: 0.5400\n",
      "****************************************************************************************************\n",
      "epoch:33/50\n",
      "training: loss:   0.8799, accuracy: 0.6780\n",
      "validation: loss: 1.0644, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:34/50\n",
      "training: loss:   0.9599, accuracy: 0.6720\n",
      "validation: loss: 1.1584, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:35/50\n",
      "training: loss:   0.9089, accuracy: 0.6960\n",
      "validation: loss: 1.2337, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:36/50\n",
      "training: loss:   0.8730, accuracy: 0.6640\n",
      "validation: loss: 1.0968, accuracy: 0.6200\n",
      "****************************************************************************************************\n",
      "epoch:37/50\n",
      "training: loss:   0.8679, accuracy: 0.6720\n",
      "validation: loss: 1.0167, accuracy: 0.6400\n",
      "****************************************************************************************************\n",
      "epoch:38/50\n",
      "training: loss:   0.7890, accuracy: 0.7040\n",
      "validation: loss: 0.9320, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:39/50\n",
      "training: loss:   0.7955, accuracy: 0.7220\n",
      "validation: loss: 1.1079, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:40/50\n",
      "training: loss:   0.7339, accuracy: 0.7240\n",
      "validation: loss: 1.0596, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:41/50\n",
      "training: loss:   0.7517, accuracy: 0.7320\n",
      "validation: loss: 1.0527, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:42/50\n",
      "training: loss:   0.8716, accuracy: 0.6940\n",
      "validation: loss: 1.0408, accuracy: 0.6800\n",
      "****************************************************************************************************\n",
      "epoch:43/50\n",
      "training: loss:   0.8774, accuracy: 0.6800\n",
      "validation: loss: 1.1825, accuracy: 0.6200\n",
      "****************************************************************************************************\n",
      "epoch:44/50\n",
      "training: loss:   0.8932, accuracy: 0.6760\n",
      "validation: loss: 1.2076, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:45/50\n",
      "training: loss:   0.9228, accuracy: 0.6660\n",
      "validation: loss: 1.3181, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:46/50\n",
      "training: loss:   0.8282, accuracy: 0.7120\n",
      "validation: loss: 1.1696, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:47/50\n",
      "training: loss:   0.7623, accuracy: 0.7400\n",
      "validation: loss: 1.0366, accuracy: 0.6600\n",
      "****************************************************************************************************\n",
      "epoch:48/50\n",
      "training: loss:   0.8574, accuracy: 0.7080\n",
      "validation: loss: 1.4330, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:49/50\n",
      "training: loss:   0.7828, accuracy: 0.7140\n",
      "validation: loss: 0.9395, accuracy: 0.6600\n"
     ]
    }
   ],
   "source": [
    "## about model\n",
    "num_classes = 10\n",
    "\n",
    "## about data\n",
    "data_dir = \"data\" ## You may need to specify the data_dir first\n",
    "inupt_size = 64\n",
    "batch_size = 64\n",
    "\n",
    "## about training\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "## model initialization\n",
    "model = CNN(classes = num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "model = model.to(device)\n",
    "\n",
    "## optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## data preparation\n",
    "train_loader, valid_loader = load_data(data_dir=data_dir,input_size=inupt_size, batch_size=batch_size)\n",
    "# train\n",
    "train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debc41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
